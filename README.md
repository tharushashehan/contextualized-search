# contextualized-search
This application shows how to use LLM in private local environment through embedded ggml bins like falcon. With the use of GPT4ALL wrapper on top of python lang chain framework as a pipe line, LLM model is embedded and used.
